{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pymorphy2\n",
    "import os\n",
    "import codecs\n",
    "import cPickle\n",
    "import numpy as np\n",
    "import time\n",
    "import gensim\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"tocheading\">Table of Contents</h1>\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Folders and files\n",
    "__DATA_FOLDER = os.path.join('data', 'Computerra_txt') # Folder with raw computerra corpus\n",
    "__PICKLE_FOLDER = os.path.join('data', 'pickle') # Folder with serialied clear corpus\n",
    "__CLEAR_CORPUS_FOLDER = os.path.join(__PICKLE_FOLDER, 'computerra') # Folder with serialied clear corpus\n",
    "__MODELS_FOLDER = 'models'\n",
    "__MODEL_FILE = os.path.join(__MODELS_FOLDER, 'd2v_computerra.model')\n",
    "\n",
    "# Normalize options\n",
    "__NORMALIZE_CORPUS = 1 # option to run normalization of the corpus\n",
    "\n",
    "# Doc2vec params\n",
    "__LEARN_DOC2VEC = 1\n",
    "__SPACE_SIZE = 500\n",
    "__THREADS = 2\n",
    "__WINDOW = 5\n",
    "LabeledSentence = gensim.models.doc2vec.LabeledSentence\n",
    "\n",
    "# Random Seed\n",
    "__RND_SEED = 1\n",
    "\n",
    "# Tokenization options\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "word_pattern = u'(?u)\\w+'\n",
    "tokenizer = RegexpTokenizer(word_pattern)\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "# Make dirs\n",
    "if not os.path.exists(__PICKLE_FOLDER):\n",
    "    os.mkdir(__PICKLE_FOLDER)\n",
    "if not os.path.exists(__CLEAR_CORPUS_FOLDER):\n",
    "    os.mkdir(__CLEAR_CORPUS_FOLDER)\n",
    "if not os.path.exists(__MODELS_FOLDER):\n",
    "    os.mkdir(__MODELS_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions and classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    '''\n",
    "    Function clears text content with normalization\n",
    "    \n",
    "    Args:\n",
    "    text - string with unnormalized content\n",
    "    \n",
    "    Returns:\n",
    "    normalized_text - string with normalized content\n",
    "    '''\n",
    "    tokenized_text = []\n",
    "\n",
    "    # sentence tokenizer\n",
    "    raw_sentences = sentence_tokenizer.tokenize(text.strip())\n",
    "    new_sentences = []\n",
    "    \n",
    "    for sentence in raw_sentences:\n",
    "        new_sentence = ''\n",
    "        for token in tokenizer.tokenize(sentence.strip()):\n",
    "            if not token.isdigit():\n",
    "                gram_info = morph.parse(token)\n",
    "                new_sentence += ' ' + (gram_info[0].normal_form)\n",
    "        if len(new_sentence):\n",
    "            new_sentences.append(new_sentence.strip())\n",
    "    \n",
    "    normalized_text = '. '.join(new_sentences).strip()\n",
    "    return normalized_text\n",
    "\n",
    "\n",
    "def load_computerra_corpus(filepath):\n",
    "    '''\n",
    "    Function loads computerra document corpus\n",
    "    Ignores .txt documents at 2nd level\n",
    "\n",
    "    Agrs:\n",
    "    filepath - path to corpus\n",
    "\n",
    "    Returns:\n",
    "    titles - list of documents titels\n",
    "    docs - list of documents content\n",
    "    '''\n",
    "\n",
    "    titles = []\n",
    "    docs = []\n",
    "\n",
    "    for root, directories, filenames in os.walk(filepath):       \n",
    "        for filename in filenames: \n",
    "            _, file_extension = os.path.splitext(filename)\n",
    "            if file_extension == '.txt':\n",
    "                file_to_read = os.path.join(root, filename)\n",
    "                with codecs.open(file_to_read,\n",
    "                                 mode='rb',\n",
    "                                 encoding='cp1251') as fin:\n",
    "                    \n",
    "                    content = fin.read()\\\n",
    "                                 .split('='*75)[2:-1]                       \n",
    "                    content = '. '.join(content).strip()\n",
    "                    \n",
    "                    if content.count(' ') >= 10:                    \n",
    "                        titles.append(file_to_read)\n",
    "                        docs.append(content)\n",
    "    return titles, docs\n",
    "\n",
    "\n",
    "class LabeledLineSentence(object):\n",
    "    '''\n",
    "    Class for doc2vec\n",
    "    '''\n",
    "    def __init__(self, doc_list, labels_list):\n",
    "        self.labels_list = labels_list\n",
    "        self.doc_list = doc_list\n",
    "    def __iter_old__(self):\n",
    "        for idx, doc in enumerate(self.doc_list):\n",
    "            yield LabeledSentence(doc.split(), [self.labels_list[idx]])\n",
    "    def __iter__(self):\n",
    "        for idx in np.random.choice(len(self.doc_list), size=len(self.doc_list), replace=False):\n",
    "            yield LabeledSentence(self.doc_list[idx].split(), [self.labels_list[idx]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and normalize corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running corpus normalization\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if __NORMALIZE_CORPUS or not os.path.exists(os.path.join(__CLEAR_CORPUS_FOLDER, 'docs_clear.p')):\n",
    "    print 'Running corpus normalization'\n",
    "    \n",
    "    # Tokenization options\n",
    "    morph = pymorphy2.MorphAnalyzer()\n",
    "    word_pattern = u'(?u)\\w+'\n",
    "    tokenizer = RegexpTokenizer(word_pattern)\n",
    "    sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "    # Load original content\n",
    "    titles, docs = load_computerra_corpus(__DATA_FOLDER)\n",
    "\n",
    "    # Clear it\n",
    "    docs_clear = map(normalize_text, docs)\n",
    "    \n",
    "    # Dump\n",
    "    with open(os.path.join(__CLEAR_CORPUS_FOLDER, 'docs_clear.p'), 'wb') as fout:\n",
    "        cPickle.dump(docs_clear, fout)\n",
    "    with open(os.path.join(__CLEAR_CORPUS_FOLDER, 'docs.p'), 'wb') as fout:\n",
    "        cPickle.dump(docs, fout)\n",
    "    with open(os.path.join(__CLEAR_CORPUS_FOLDER, 'titles.p'), 'wb') as fout:\n",
    "        cPickle.dump(titles, fout)\n",
    "else:\n",
    "    print 'Loading prenomalized corpus'\n",
    "    \n",
    "    # Load\n",
    "    with open(os.path.join(__CLEAR_CORPUS_FOLDER, 'docs_clear.p'), 'rb') as fin:\n",
    "        docs_clear = cPickle.load(fin)\n",
    "    with open(os.path.join(__CLEAR_CORPUS_FOLDER, 'docs.p'), 'rb') as fin:\n",
    "        docs = cPickle.load(fin)\n",
    "    with open(os.path.join(__CLEAR_CORPUS_FOLDER, 'titles.p'), 'rb') as fin:\n",
    "        titles = cPickle.load(fin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "if __LEARN_DOC2VEC or not os.path.exists(__MODEL_FILE):\n",
    "\n",
    "    print 'Learning Doc2Vec model..'\n",
    "    \n",
    "    # remove dots in document\n",
    "    docs_clear_nodots = []\n",
    "    for doc in docs_clear:\n",
    "        docs_clear_nodots.append(doc.replace('.', ''))\n",
    "    \n",
    "    # preparing input\n",
    "    sentences = LabeledLineSentence(docs_clear_nodots, titles)\n",
    "    \n",
    "    # setting model params\n",
    "    model = gensim.models.Doc2Vec(dm=1, size=__SPACE_SIZE, window=__WINDOW, min_count=3,\n",
    "                                  workers=__THREADS, negative=10, sample=1e-5, hs=0, seed=__RND_SEED)\n",
    "    \n",
    "    model.build_vocab(sentences)\n",
    "    \n",
    "    # learining \n",
    "    alpha, min_alpha, passes = (0.025, 0.001, 30)\n",
    "    alpha_delta = (alpha - min_alpha) / passes\n",
    "    \n",
    "    for epoch in range(passes):\n",
    "        start_time = time.time()\n",
    "        print 'alpha = %f' % (alpha)\n",
    "        model.alpha = alpha\n",
    "        model.min_alpha = alpha # fix the learning rate, \n",
    "        model.train(sentences)\n",
    "        print 'epoch %d/%d done in %f seconds' % (epoch+1, passes, time.time()-start_time)\n",
    "\n",
    "        alpha -= alpha_delta\n",
    "        \n",
    "        \n",
    "    # Dump\n",
    "    model.save(__MODEL_FILE)\n",
    "    \n",
    "else:\n",
    "    \n",
    "    print 'Loading Doc2Vec model..'\n",
    "    \n",
    "    model = gensim.models.Doc2Vec.load(__MODEL_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEMO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity with documents from corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pick random document from corpus\n",
    "idx_title = np.random.randint(model.docvecs.count)\n",
    "doc_id = titles[idx_title]\n",
    "idx = titles.index(doc_id)\n",
    "\n",
    "sims = model.docvecs.most_similar(doc_id, topn=3)  # get similar documents\n",
    "print u'Query (%s):\\n «%s...»\\n' % (doc_id, docs[idx][:1000]) \n",
    "print u'='*30\n",
    "print u'TOP-3 similar documents %s:\\n'\n",
    "\n",
    "for label, index in [('top-1', 0), ('top-2', 1), ('top-3', 2)]:\n",
    "    idx = titles.index(sims[index][0])    \n",
    "    print(u'%s - %s, Similarity=%f:\\n «%s...»\\n' % (label,\n",
    "                                                    sims[index][0],\n",
    "                                                    sims[index][1],\n",
    "                                                    docs[idx][:1000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity with arbitrary document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_to_read = 'data/Computerra_txt/1999/285/2345/index.txt'\n",
    "\n",
    "# Loading raw file content\n",
    "with codecs.open(file_to_read,\n",
    "                 mode='rb',\n",
    "                 encoding='cp1251') as fin:\n",
    "                    raw_content = fin.read()\\\n",
    "                                 .split('='*75)[2:-1]                       \n",
    "                    raw_content = '. '.join(raw_content).strip()\n",
    "\n",
    "content = normalize_text(raw_content).replace('.', '')\n",
    "inferred_doc = model.infer_vector(content.split())\n",
    "\n",
    "sims = model.docvecs.most_similar([inferred_doc], topn=3)  # get similar documents\n",
    "print u'Query:\\n «%s»\\n' % (raw_content[:1000]) \n",
    "print u'='*30\n",
    "print u'TOP-3 similar documents %s:\\n'\n",
    "\n",
    "for label, index in [('top-1', 0), ('top-2', 1), ('top-3', 2)]:\n",
    "    idx = titles.index(sims[index][0])    \n",
    "    print(u'%s - %s, Similarity=%f:\\n «%s»\\n' % (label,\n",
    "                                                 sims[index][0],\n",
    "                                                 sims[index][1],\n",
    "                                                 docs[idx][:1000]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
