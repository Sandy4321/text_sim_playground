{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pymorphy2\n",
    "import os\n",
    "import codecs\n",
    "import cPickle\n",
    "import numpy as np\n",
    "import time\n",
    "import gensim\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import pairwise_distances as sk_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"tocheading\">Table of Contents</h1>\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Folders and files\n",
    "__DATA_FOLDER = os.path.join('data', 'news') # Folder with raw computerra corpus\n",
    "__PICKLE_FOLDER = os.path.join('data', 'pickle') # Folder with serialied clear corpus\n",
    "__CLEAR_CORPUS_FOLDER = os.path.join(__PICKLE_FOLDER, 'news') # Folder with serialied clear corpus\n",
    "__MODELS_FOLDER = 'models'\n",
    "\n",
    "# Normalize options\n",
    "__NORMALIZE_CORPUS = 0 # option to run normalization of the corpus\n",
    "\n",
    "# Random Seed\n",
    "__RND_SEED = 1\n",
    "\n",
    "# Tokenization options\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "word_pattern = u'(?u)\\w+'\n",
    "tokenizer = RegexpTokenizer(word_pattern)\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "# Make dirs\n",
    "if not os.path.exists(__PICKLE_FOLDER):\n",
    "    os.mkdir(__PICKLE_FOLDER)\n",
    "if not os.path.exists(__CLEAR_CORPUS_FOLDER):\n",
    "    os.mkdir(__CLEAR_CORPUS_FOLDER)\n",
    "if not os.path.exists(__MODELS_FOLDER):\n",
    "    os.mkdir(__MODELS_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions and classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    '''\n",
    "    Function clears text content with normalization\n",
    "    \n",
    "    Args:\n",
    "    text - string with unnormalized content\n",
    "    \n",
    "    Returns:\n",
    "    normalized_text - string with normalized content\n",
    "    '''\n",
    "    tokenized_text = []\n",
    "\n",
    "    # sentence tokenizer\n",
    "    raw_sentences = sentence_tokenizer.tokenize(text.strip())\n",
    "    new_sentences = []\n",
    "    \n",
    "    for sentence in raw_sentences:\n",
    "        new_sentence = ''\n",
    "        for token in tokenizer.tokenize(sentence.strip()):\n",
    "            if not token.isdigit():\n",
    "                gram_info = morph.parse(token)\n",
    "                new_sentence += ' ' + (gram_info[0].normal_form)\n",
    "        if len(new_sentence):\n",
    "            new_sentences.append(new_sentence.strip())\n",
    "    \n",
    "    normalized_text = '. '.join(new_sentences).strip()\n",
    "    return normalized_text\n",
    "\n",
    "\n",
    "def load_computerra_corpus(filepath):\n",
    "    '''\n",
    "    Function loads computerra document corpus\n",
    "    Ignores .txt documents at 2nd level\n",
    "\n",
    "    Agrs:\n",
    "    filepath - path to corpus\n",
    "\n",
    "    Returns:\n",
    "    titles - list of documents titels\n",
    "    docs - list of documents content\n",
    "    '''\n",
    "\n",
    "    titles = []\n",
    "    docs = []\n",
    "\n",
    "    for root, directories, filenames in os.walk(data_folder):       \n",
    "        for filename in filenames: \n",
    "            _, file_extension = os.path.splitext(filename)\n",
    "            if file_extension == '.txt':\n",
    "                file_to_read = os.path.join(root, filename)\n",
    "                with codecs.open(file_to_read,\n",
    "                                 mode='rb',\n",
    "                                 encoding='cp1251') as fin:\n",
    "                    \n",
    "                    content = fin.read()\\\n",
    "                                 .split('='*75)[2:-1]                       \n",
    "                    content = '. '.join(content).strip()\n",
    "                    \n",
    "                    if content.count(' ') >= 10:                    \n",
    "                        titles.append(file_to_read)\n",
    "                        docs.append(content)\n",
    "    return titles, docs\n",
    "\n",
    "\n",
    "class LabeledLineSentence(object):\n",
    "    '''\n",
    "    Class for doc2vec\n",
    "    '''\n",
    "    def __init__(self, doc_list, labels_list):\n",
    "        self.labels_list = labels_list\n",
    "        self.doc_list = doc_list\n",
    "    def __iter_old__(self):\n",
    "        for idx, doc in enumerate(self.doc_list):\n",
    "            yield LabeledSentence(doc.split(), [self.labels_list[idx]])\n",
    "    def __iter__(self):\n",
    "        for idx in np.random.choice(len(self.doc_list), size=len(self.doc_list), replace=False):\n",
    "            yield LabeledSentence(self.doc_list[idx].split(), [self.labels_list[idx]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and normalize corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading prenomalized corpus\n",
      "Wall time: 26 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if __NORMALIZE_CORPUS or not os.path.exists(os.path.join(__CLEAR_CORPUS_FOLDER, 'docs_clear.p')):\n",
    "    print 'Running corpus normalization'\n",
    "    \n",
    "    # Tokenization options\n",
    "    morph = pymorphy2.MorphAnalyzer()\n",
    "    word_pattern = u'(?u)\\w+'\n",
    "    tokenizer = RegexpTokenizer(word_pattern)\n",
    "    sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "    # Load original content\n",
    "    titles, docs = load_computerra_corpus(__DATA_FOLDER)\n",
    "\n",
    "    # Clear it\n",
    "    docs_clear = map(normalize_text, docs)\n",
    "    \n",
    "    # Dump\n",
    "    with open(os.path.join(__CLEAR_CORPUS_FOLDER, 'docs_clear.p'), 'wb') as fout:\n",
    "        cPickle.dump(docs_clear, fout)\n",
    "    with open(os.path.join(__CLEAR_CORPUS_FOLDER, 'docs.p'), 'wb') as fout:\n",
    "        cPickle.dump(docs, fout)\n",
    "    with open(os.path.join(__CLEAR_CORPUS_FOLDER, 'titles.p'), 'wb') as fout:\n",
    "        cPickle.dump(titles, fout)\n",
    "else:\n",
    "    print 'Loading prenomalized corpus'\n",
    "    \n",
    "    # Load\n",
    "    with open(os.path.join(__CLEAR_CORPUS_FOLDER, 'docs_clear.p'), 'rb') as fin:\n",
    "        docs_clear = cPickle.load(fin)\n",
    "    with open(os.path.join(__CLEAR_CORPUS_FOLDER, 'docs.p'), 'rb') as fin:\n",
    "        docs = cPickle.load(fin)\n",
    "    with open(os.path.join(__CLEAR_CORPUS_FOLDER, 'titles.p'), 'rb') as fin:\n",
    "        titles = cPickle.load(fin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running vectorization\n",
      "(32083, 29300)\n",
      "Running LSA\n",
      "Wall time: 1min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# stopwords modifications\n",
    "stopwords_rus = stopwords.words('russian')\n",
    "\n",
    "# TF-IDF vectorizer\n",
    "vect = TfidfVectorizer(stop_words=stopwords_rus, \n",
    "                       binary=False,\n",
    "                       ngram_range=(1,1),\n",
    "                       norm='l2',\n",
    "                       sublinear_tf=False,\n",
    "                       min_df=5)\n",
    "\n",
    "print 'Running vectorization'\n",
    "X = vect.fit_transform(docs_clear)\n",
    "print X.shape\n",
    "\n",
    "# LSA\n",
    "lsa = TruncatedSVD(n_components=400, algorithm='arpack')\n",
    "\n",
    "print 'Running LSA'\n",
    "Y = lsa.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEMO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity with documents from corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query (data\\news\\080404week\\newsDump\\100681):\n",
      " «Заказчик убийства Сергея Юшенкова не хочет сидеть 20 лет\n",
      "Адвокат Михаила Коданева, приговоренного к 20 годам лишения свободы по делу об убийстве депутата Сергея Юенкова, утверждает, что его подзхащитный пытался покончить жизнь самоубийством.\n",
      "\"Я распологаю сведениями, что Коданев пытался покончить жизнь самоубийством и был госпитализирован в Институт Склифосовского\", - сказал РИА \"Новости\" адвокат Генри Резник.\n",
      "Он не смог уточнить, когда именно Коданев предпринял попытку самоубийства:\n",
      "\"Это случилось в день оглашения приговора или накануне\".\n",
      "Резник также ничего не сказал о состоянии здоровья Коданева в настоящее время.\n",
      "\"Он переведен в медчасть Бутырского изолятора, сейчас к нему пытается пробиться еще один адвокат\", - сказал адвокат.\n",
      "Ранее источник в ГУИН Минюста сообщил РИА \"Новости\", что состояние здоровья Коданева ухудшилось накануне во второй половине дня, после чего он был помещен в медчасть.\n",
      "О причинах ухудшения здоровья Коданева ничего сказано не было.\n",
      "Во вторник, 30 марта, Михаил...»\n",
      "\n",
      "==============================\n",
      "TOP-3 similar documents %s:\n",
      "\n",
      "top-1 - data\\news\\080404week\\newsDump\\94635, Similarity=0.969705:\n",
      " «Михаил Коданев пытался покончить с собой\n",
      "Михаил Коданев, приговоренный к 20 годам лишения свободы по делу об убийстве депутата Сергея Юшенкова, пытался покончить жизнь самоубийством, утверждает его адвокат Генри Резник.\n",
      "\"Я располагаю сведениями, что Коданев пытался покончить жизнь самоубийством и был госпитализирован в Институт Склифосовского\", - сказал РИА \"Новости\" Резник.\n",
      "Он не смог уточнить, когда именно Коданев совершил попытку самоубийства, \"это случилось в день оглашения приговора или накануне\".\n",
      "\"Он переведен в медчасть Бутырского изолятора, сейчас к нему пытается пробиться еще один адвокат\", - сказал Резник.\n",
      "Сопреседатель партии \"Либеральная Россия\" из числа сторонников Бориса Березовского Михаил Коданев в момент оглашения приговора в суде 30 марта отсутствовал.\n",
      "По одним данным, у Коданева случилось пищевое отравление.\n",
      "Однако в ряде СМИ появились предположения, что Коданев пытался покончить с собой.\n",
      "Cостояние здоровья Коданева ухудшилось накануне во второй половине дня, после ч...»\n",
      "\n",
      "top-2 - data\\news\\080404week\\newsDump\\100041, Similarity=0.962282:\n",
      " «Резник: Коданев пытался покончить с собой\n",
      "Осужденный за организацию убийства депутата Госдумы Сергея Юшенкова Михаил Коданев пытался покончить жизнь самоубийством.\n",
      "Об этом РБК сообщил адвокат Коданева Генри Резник.\n",
      "По словам юриста, в минувший вторник Коданев отсутствовал на оглашении приговора, поскольку был доставлен в институт им.Склифософского в состоянии \"сильного психического расстройства, сопровождавшегося попыткой суицида\".\n",
      "В настоящее время, по словам адвоката, Коданев находится в тюремной больнице Бутырского СИЗО, его состояние врачи оценивают как удовлетворительное.\n",
      "Предположения о возможной попытке самоубийства появились в СМИ еще в день оглашения приговора.\n",
      "В ГУИН их опровергли и заявили, что у подсудимого в этот день сильно подскочило давление.\n",
      "Жена Коданева выразила недоумение по поводу этой версии, так как, по ее сведениям, ее мужа возили в суд и с повышенным давлением.\n",
      "Ссылаясь на знакомых, она рассказала, что 30 марта Коданев был срочно госпитализирован в НИИ Склифосо...»\n",
      "\n",
      "top-3 - data\\news\\080404week\\newsDump\\90923, Similarity=0.954773:\n",
      " «Коданев пытался покончить с собой\n",
      "Организатор убийства депутата Госдумы Сергея Юшенкова Михаил Коданев пытался покончить жизнь самоубийством.\n",
      "Как сообщил его адвокат Генри Резник, 30 марта Коданев отсутствовал на оглашении приговора, поскольку был доставлен в институт им.Склифософского в состоянии \"сильного психического расстройства, сопровождавшегося попыткой суицида\", передает РБК.\n",
      "В настоящее время, по словам адвоката, Коданев находится в тюремной больнице Бутырского СИЗО, его состояние врачи оценивают как удовлетворительное....»\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:386: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and willraise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Pick random document from corpus\n",
    "i_doc = np.random.randint(Y.shape[0])\n",
    "doc_id = titles[i_doc]\n",
    "sims = sk_dist(Y[i_doc,:], Y=Y, metric='cosine').ravel()\n",
    "\n",
    "sims_argsort = np.argsort(sims)\n",
    "\n",
    "print u'Query (%s):\\n «%s...»\\n' % (doc_id, docs[i_doc][:1000]) \n",
    "print u'='*30\n",
    "print u'TOP-3 similar documents %s:\\n'\n",
    "\n",
    "\n",
    "for label, index in [('top-1', 1), ('top-2', 2), ('top-3', 3)]:\n",
    "    print(u'%s - %s, Similarity=%f:\\n «%s...»\\n' % (label,\n",
    "                                                    titles[sims_argsort[index]],\n",
    "                                                    1-sims[sims_argsort[index]],\n",
    "                                                    docs[sims_argsort[index]][:1000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity with arbitrary document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      " «Счета международного аэропорта Толмачево арестованы\n",
      "Сегодня служба судебных приставов отказала Толмачево в отложении сроков исполнительных действий по взысканию с аэропорта 102 млн. рублей в счет долга перед Российским авиационным консорциумом.\n",
      "Об этом корреспонденту ИА REGNUM сообщил финансовый директор Сергей Ченчиков.\n",
      "Приставы побывали в аэропорте, изъяли из кассы определенную сумму и наложили аресты на счета аэропорта, послав соответствующие предписания в банки о прекращении движения по счетам.\n",
      "По словам гендиректора это обстоятельство практически дестабилизирует работу аэропорта.\n",
      "Менеджер высказал беспокойство тем, что аэропорт лишится возможности нормального функционирования и обеспечения топливом и электроэнергией.\n",
      "\"Руководство аэропорта попытается добиться переносов срока исполнительных действий по взысканию долгов еще до того, как арест счетов скажется на работе порта\", - заверил Сергей Ченчиков.\n",
      "Напомним, что сегодня приставы выполнили августовское решение кассационной инстан...»\n",
      "\n",
      "==============================\n",
      "TOP-3 similar documents %s:\n",
      "\n",
      "top-1 - data\\news\\shevardWeek\\newsDump\\100, Similarity=1.000000:\n",
      " «Счета международного аэропорта Толмачево арестованы\n",
      "Сегодня служба судебных приставов отказала Толмачево в отложении сроков исполнительных действий по взысканию с аэропорта 102 млн. рублей в счет долга перед Российским авиационным консорциумом.\n",
      "Об этом корреспонденту ИА REGNUM сообщил финансовый директор Сергей Ченчиков.\n",
      "Приставы побывали в аэропорте, изъяли из кассы определенную сумму и наложили аресты на счета аэропорта, послав соответствующие предписания в банки о прекращении движения по счетам.\n",
      "По словам гендиректора это обстоятельство практически дестабилизирует работу аэропорта.\n",
      "Менеджер высказал беспокойство тем, что аэропорт лишится возможности нормального функционирования и обеспечения топливом и электроэнергией.\n",
      "\"Руководство аэропорта попытается добиться переносов срока исполнительных действий по взысканию долгов еще до того, как арест счетов скажется на работе порта\", - заверил Сергей Ченчиков.\n",
      "Напомним, что сегодня приставы выполнили августовское решение кассационной инстан...»\n",
      "\n",
      "top-2 - data\\news\\vyboryWeek\\newsDump\\5769, Similarity=0.931320:\n",
      " «В Новосибирске началась процедура ареста здания аэровокзала\n",
      "Сегодня в Новосибирске началась процедура ареста одного из двух зданий аэровокзалов, принадлежащих ОАО \"Аэропорт Толмачево\".\n",
      "Об этом корреспонденту ИА REGNUM сообщил финансовый директор международного аэропорта Сергей Ченчиков.\n",
      "По его словам, сегодня здание внутренних авиалиний посетили судебные приставы, которые осмотрели его и сделали опись.\n",
      "За этими действиями, по его словам, могут последовать последствия, которые повлияют на судьбу аэропорта, в частности, не исключена дальнейшая продажа аэровокзала заинтересованным структурам.\n",
      "Арест одного из двух зданий аэровокзалов аэропорта последовал за арестом счетов ОАО, который был предпринят неделю назад.\n",
      "Тогда служба судебных приставов отказала Толмачево в отложении сроков исполнительных действий по взысканию с аэропорта 102 млн. рублей в счет долга перед Российским авиационным консорциумом и наложила аресты на счета аэропорта, послав соответствующие предписания в банки о прекраще...»\n",
      "\n",
      "top-3 - data\\news\\shevardWeek\\newsDump\\4461, Similarity=0.754016:\n",
      " «Внуково может стать главным пересадочным пунктом Москвы - сейчас на его долю приходится только 12-15% объемов авиасообщений столицы\n",
      "По данным Государственной службы гражданской авиации Минтранса России, за последние годы число аэропортов в стране снизилось с 1302 до 451.\n",
      "Опорную сеть образуют 63 аэропорта федерального значения.\n",
      "На их долю приходится около 85% объемов авиасообщений по внутренним линиям и более 90% - международным.\n",
      "При этом половина всех авиапассажирских и грузовых перевозок России осуществляется через Московский авиаузел.\n",
      "В Европе он занимает пока десятое место по пассажиропотоку.\n",
      "Но уже в прошлом году продемонстрировал рекордный рост - 8,1%.\n",
      "Какой же на сегодня расклад \"сил\" воздушных гаваней столицы?\n",
      "Как считают эксперты, доля аэропорта Шереметьево составляет примерно 48-49%, Внуково - 12-15%, Домодедово - 36-39%.\n",
      "Специалисты не скрывают: мощности технологических комплексов этих аэропортов достаточны, но используются нерационально.\n",
      "Это мешает созданию в Москве \"хаба\" ...»\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_to_read = 'data/news/shevardWeek/newsDump/100'\n",
    "\n",
    "# Loading raw file content\n",
    "with codecs.open(file_to_read,\n",
    "                 mode='rb',\n",
    "                 encoding='cp1251') as fin:\n",
    "                    raw_content = fin.read().strip()\n",
    "\n",
    "content = normalize_text(raw_content)\n",
    "content_vect = vect.transform([content])\n",
    "content_lsa = lsa.transform(content_vect)\n",
    "\n",
    "sims = sk_dist(content_lsa, Y=Y, metric='cosine').ravel()\n",
    "\n",
    "sims_argsort = np.argsort(sims)\n",
    "\n",
    "print u'Query:\\n «%s...»\\n' % (raw_content[:1000]) \n",
    "print u'='*30\n",
    "print u'TOP-3 similar documents %s:\\n'\n",
    "\n",
    "\n",
    "for label, index in [('top-1', 0), ('top-2', 1), ('top-3', 2)]:\n",
    "    print(u'%s - %s, Similarity=%f:\\n «%s...»\\n' % (label,\n",
    "                                                    titles[sims_argsort[index]],\n",
    "                                                    1-sims[sims_argsort[index]],\n",
    "                                                    docs[sims_argsort[index]][:1000]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
